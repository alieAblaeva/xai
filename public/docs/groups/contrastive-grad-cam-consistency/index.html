<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Consistent Explanations by Contrastive Learning # Introduction: Unveiling the Black Box of Deep Learning # Demystifying Decisions with Post-hoc Explanations # Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained
These methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model&rsquo;s prediction. High values correspond to the regions that took important role in the network&rsquo;s decision."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/contrastive-grad-cam-consistency/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="XAI"><meta property="og:description" content="Consistent Explanations by Contrastive Learning # Introduction: Unveiling the Black Box of Deep Learning # Demystifying Decisions with Post-hoc Explanations # Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained
These methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model’s prediction. High values correspond to the regions that took important role in the network’s decision."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Contrastive Grad Cam Consistency | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.650f6205378dcbfd5a21a9d8c8d48f949faff1fe9ca02176d547f121a78ceec4.js integrity="sha256-ZQ9iBTeNy/1aIanYyNSPlJ+v8f6coCF21UfxIaeM7sQ=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/ class=active>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Contrastive Grad Cam Consistency</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#consistent-explanations-by-contrastive-learning>Consistent Explanations by Contrastive Learning</a><ul><li><a href=#introduction-unveiling-the-black-box-of-deep-learning>Introduction: Unveiling the Black Box of Deep Learning</a><ul><li><a href=#demystifying-decisions-with-post-hoc-explanations>Demystifying Decisions with Post-hoc Explanations</a></li><li><a href=#the-challenge-of-spatial-transformations>The Challenge of Spatial Transformations</a></li><li><a href=#fine-grained-classification-a-case-for-explainability>Fine-Grained Classification: A Case for Explainability</a></li><li><a href=#suggested-solution-enhancing-grad-cam-for-spatial-consistency>Suggested solution: Enhancing Grad-CAM for Spatial Consistency</a></li><li><a href=#key-ideas-guiding-principles-for-explainability>Key ideas: Guiding Principles for Explainability</a></li><li><a href=#metrics>Metrics</a></li><li><a href=#insights-and-benefits>Insights and Benefits</a></li></ul></li><li><a href=#method>Method</a><ul><li><a href=#background-on-base-explanation-methods>Background on Base Explanation Methods</a></li><li><a href=#contrastive-grad-cam-consistency-loss>Contrastive Grad-CAM Consistency Loss</a></li></ul></li><li><a href=#our-implementation>Our Implementation</a><ul><li><a href=#google-colab-notebookhttpscolabresearchgooglecomdrive1we3vlkexppim5wlimtjh4w_tvi7yikhouspsharing><a href="https://colab.research.google.com/drive/1we3vLKeXpPim5wLiMTjh4w_tVI7YIKHo?usp=sharing">Google Colab Notebook</a></a></li></ul></li><li><a href=#resources>Resources</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=consistent-explanations-by-contrastive-learning>Consistent Explanations by Contrastive Learning
<a class=anchor href=#consistent-explanations-by-contrastive-learning>#</a></h1><h2 id=introduction-unveiling-the-black-box-of-deep-learning>Introduction: Unveiling the Black Box of Deep Learning
<a class=anchor href=#introduction-unveiling-the-black-box-of-deep-learning>#</a></h2><h3 id=demystifying-decisions-with-post-hoc-explanations>Demystifying Decisions with Post-hoc Explanations
<a class=anchor href=#demystifying-decisions-with-post-hoc-explanations>#</a></h3><p>Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained</p><p>These methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model&rsquo;s prediction. High values correspond to the regions that took important role in the network&rsquo;s decision.</p><h3 id=the-challenge-of-spatial-transformations>The Challenge of Spatial Transformations
<a class=anchor href=#the-challenge-of-spatial-transformations>#</a></h3><p>However, many interpretation methods falter when faced with spatial transformations of images. Shifting, zooming, rotating, or shearing an image can significantly alter the explanation heatmap, even though the image content remains essentially the same. This inconsistency raises concerns about the reliability and robustness of these explanation methods.</p><h3 id=fine-grained-classification-a-case-for-explainability>Fine-Grained Classification: A Case for Explainability
<a class=anchor href=#fine-grained-classification-a-case-for-explainability>#</a></h3><p>Authors of the paper pay special attention to the fine-grained image classification task, where the goal is to distinguish between subtle differences within a broader category. For instance, we might want to classify different breeds of dogs or species of birds. In such tasks, understanding the model&rsquo;s reasoning becomes crucial for building trust and ensuring fairness.</p><h3 id=suggested-solution-enhancing-grad-cam-for-spatial-consistency>Suggested solution: Enhancing Grad-CAM for Spatial Consistency
<a class=anchor href=#suggested-solution-enhancing-grad-cam-for-spatial-consistency>#</a></h3><p>Authors propose an approach to improve Grad-CAM, making its explanations more stable across spatial transformations. Inspired by contrastive self-supervised learning, they introduce a novel loss function that leverages unlabeled data during training.</p><h3 id=key-ideas-guiding-principles-for-explainability>Key ideas: Guiding Principles for Explainability
<a class=anchor href=#key-ideas-guiding-principles-for-explainability>#</a></h3><p>Adopt ideas from contrastive self-supervised learning and design a loss function that will allow to train on unlabeled data.</p><p>The loss function encourages two key properties:</p><ul><li><strong>Consistency:</strong> The Grad-CAM heatmap of an image should be similar to the heatmap of its augmented versions (e.g., zoomed, rotated).</li><li><strong>Distinctiveness:</strong> The Grad-CAM heatmap of an image should be different from the heatmaps of other, random images.</li></ul><p>This approach ensures that the explanations focus on the inherent content of the image rather than its spatial arrangement.</p><h3 id=metrics>Metrics
<a class=anchor href=#metrics>#</a></h3><p>To assess the quality of the explanations, authors utilize classification accuracy, Content Heatmaps and Contrastive Grad-CAM Consistency Loss.</p><h4 id=classification-accuracy>Classification Accuracy
<a class=anchor href=#classification-accuracy>#</a></h4><p>Authors utilize classification accuracy during training process. We will use it as our main metrics along with CGC Loss during evaluation since we do not have annotated samples to calculate Content Heatmap scores.</p><h4 id=content-heatmap>Content Heatmap
<a class=anchor href=#content-heatmap>#</a></h4><p>Content Heatmaps are annotated by humans. They indicate the regions of importance within an image. By comparing the model-generated heatmaps with the Content Heatmaps, it is possible to evaluate the accuracy and faithfulness of the explanations.</p><h4 id=cgc-loss>CGC Loss
<a class=anchor href=#cgc-loss>#</a></h4><p>Contrastive Grad-CAM Consistency Loss is also used by authors to identify that proposed method generalizes to unseen data as well.</p><h3 id=insights-and-benefits>Insights and Benefits
<a class=anchor href=#insights-and-benefits>#</a></h3><p>The proposed method&rsquo;s authors state that it demonstrates several advantages:</p><ul><li><strong>Improved Accuracy:</strong> It leads to better performance in fine-grained classification tasks.</li><li><strong>Unlabeled Data Utilization:</strong> It can leverage the abundance of unlabeled data for training.</li><li><strong>Enhanced Consistency:</strong> It generates explanations that are more robust to spatial transformations.</li><li><strong>Regularization Effect:</strong> It acts as a regularizer, leading to better generalization and performance even with limited labeled data.</li></ul><h2 id=method>Method
<a class=anchor href=#method>#</a></h2><h3 id=background-on-base-explanation-methods>Background on Base Explanation Methods
<a class=anchor href=#background-on-base-explanation-methods>#</a></h3><h4 id=cam>CAM
<a class=anchor href=#cam>#</a></h4><p><img src=/CGC/CAM.png alt="Class Activation Mapping"></p><h5 id=definition>Definition
<a class=anchor href=#definition>#</a></h5><p>Class Activation Mapping, or CAM for short, is a special technique used in computer vision and machine learning. It helps us understand how a Convolutional Neural Network (CNN) makes decisions when classifying images. Basically, CAM lets us visualize which parts of an image are most important for the network&rsquo;s prediction.</p><h5 id=how-does-cam-work>How does CAM work?
<a class=anchor href=#how-does-cam-work>#</a></h5><ol><li><strong>Modify the CNN:</strong> We take a trained CNN and remove the fully connected layers at the end. Instead, we add a Global Average Pooling (GAP) layer after the last convolutional layer.</li><li><strong>Global Average Pooling:</strong> GAP takes each feature map from the last convolutional layer and calculates the average of all values within that map. This results in a single representative value for each feature map.</li><li><strong>Prediction with a single layer:</strong> These averaged values are then fed into a single fully connected layer with as many outputs as there are classes in our problem. This layer learns to predict the image class based on the feature map averages.</li><li><strong>Weights and Importance:</strong> The weights in this final layer tell us how important each feature map is for each class.</li><li><strong>Creating the heatmap:</strong> We multiply these weights with the corresponding feature maps from the last convolutional layer. This creates a weighted sum for each class, highlighting the regions in the feature maps that were most influential for that class.</li><li><strong>Visualization:</strong> These weighted sums are then visualized as heatmaps, showing which parts of the image contributed most to the predicted class. This heatmap provides a visual explanation of the CNN&rsquo;s decision-making process.</li></ol><h4 id=grad-cam>Grad-CAM
<a class=anchor href=#grad-cam>#</a></h4><p><img src=/CGC/grad-cam.png alt=Grad-CAM></p><p>Grad-CAM, which stands for Gradient-weighted Class Activation Mapping, is another technique similar to CAM that helps us visualize what a CNN is &ldquo;looking at&rdquo; when making a prediction.</p><p>One key advantage of Grad-CAM is that it doesn&rsquo;t require modifying the original CNN architecture, unlike CAM which needs the Global Average Pooling layer. This makes Grad-CAM applicable to a wider range of CNN models.</p><h5 id=how-does-grad-cam-work>How does Grad-CAM work?
<a class=anchor href=#how-does-grad-cam-work>#</a></h5><ol><li><strong>Forward Pass and Prediction:</strong> We start by feeding an image into the CNN and obtaining the class prediction.</li><li><strong>Gradient Calculation:</strong> We then calculate the gradient of the score for the predicted class with respect to the feature maps of the last convolutional layer. This tells us how much each feature map activation influences the final prediction score.</li><li><strong>ReLU and Global Average Pooling:</strong> We apply a ReLU function to the gradients to focus on the features that have a positive influence on the class score. Then, for each feature map, we take the average of these positive gradients across all the spatial locations in that map. This gives us a single value representing the importance of each feature map for the predicted class.</li><li><strong>Weighted Combination:</strong> We then use these averaged gradients as weights and combine them with the corresponding feature maps from the last convolutional layer. This creates a weighted sum that highlights the important regions in the feature maps for the predicted class.</li><li><strong>Visualization as Heatmap:</strong> Finally, we visualize this weighted sum as a heatmap, superimposed on the original image. This heatmap shows which parts of the image were most influential in the CNN&rsquo;s decision-making process.</li></ol><h3 id=contrastive-grad-cam-consistency-loss>Contrastive Grad-CAM Consistency Loss
<a class=anchor href=#contrastive-grad-cam-consistency-loss>#</a></h3><p>We want the transformed interpretation of a query image to be close to the interpretation of the transformed query while being far from interpretations of other random images</p><p>To understand the main formula, let&rsquo;s first define the key elements involved:</p><ul><li><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(g(\cdot)\)
</span>: Grad-CAM operator that produces interpretation heatmap</li><li><span>\(\{X_j\}^n_{j=1}\)
</span>: Set of <span>\(n\)
</span>random images</li><li><span>\(\Tau_j(\cdot)\)
</span>: Independent random spacial transformation. This transformation could involve scaling, cropping, and/or flipping the image.</li><li><span>\(\{g_j(T_j(j_j))\}^n_{j=1}\)
</span>: Grad-CAM heatmaps of the augmented images</li><li><span>\(x_i\ where\ i\in 1..n\)
</span>: Query image</li><li><span>\(\Tau_i(g_i(x_i))\)
</span>: We apply the same transformation we had applied to <span>\(x_i\)
</span>to the Grad-CAM heatmap instead of the image</li></ul><p>We want <span>\(\Tau_i(g_i(x_i))\)
</span>to be close to <span>\(g_i(\Tau_i(x_i))\)
</span>and far from <span>\(\{g_j(\Tau_j(x_j))\}_{j\neq i}\)
</span>Hence, we define the following loss function:</p><p><span>\[L_i = -\log{\frac{\exp(\text{sim}(\Tau_i(g_i(x_i))), g_i(\Tau_i(x_i))/\tau)}{\sum^n_{j=1}\exp(\text{sim}(\Tau_i(g_i(x_i))), g_j(\Tau_j(x_j)))}}\]
</span>where <span>\(\tau\)
</span>is the temperature hyperparameter and <span>\(\text{sim}(a, b)\)
</span>is a similarity function. In the experiments cosine similarity was used.</p><p>Since we want to optimize training we will assume that each image is the query once in one mini-batch. Thus we define our contrastive Grad-CAM consistency loss as:
<span>\[L_{CGC} = \sum_i{L_i}\]</span></p><p>Our final loss will be a combination of the cross entropy loss with defined consistency loss:
<span>\[L = L_{CE} + \lambda L_{CGC}\]
</span>where <span>\(\lambda\)
</span>is a hyperparameter that controls trade-off between usual supervised way of training and self-supervised method that allows to train our model on unlabeled data using pseudo labels (image labels).</p><p>Here you can see visual description of the approach described above:
<img src=/CGC/cgc.png alt=CGC></p><h2 id=our-implementation>Our Implementation
<a class=anchor href=#our-implementation>#</a></h2><h3 id=google-colab-notebookhttpscolabresearchgooglecomdrive1we3vlkexppim5wlimtjh4w_tvi7yikhouspsharing><a href="https://colab.research.google.com/drive/1we3vLKeXpPim5wLiMTjh4w_tVI7YIKHo?usp=sharing">Google Colab Notebook</a>
<a class=anchor href=#google-colab-notebookhttpscolabresearchgooglecomdrive1we3vlkexppim5wlimtjh4w_tvi7yikhouspsharing>#</a></h3><p>This section describes our implementation of the CGC method using PyTorch. Here&rsquo;s a concise overview:</p><ol><li><p><strong>Data Loading and Transformation:</strong></p><ul><li>We use the Imagenette dataset for demonstration purposes (CUB dataset is also available in the code).</li><li>A custom <code>ContrastiveTransforms</code> class handles data augmentation, including random resized cropping and horizontal flips.</li><li>Data loaders are set up for both labeled and unlabeled data.</li></ul></li><li><p><strong>Model Definition:</strong></p><ul><li>The <code>CGC_Model</code> class utilizes a ResNet-18 backbone.</li><li>A forward hook is applied to the last convolutional layer (<code>layer4</code>) to extract feature maps for Grad-CAM computation.</li><li>The forward pass implements the CGC logic, including Grad-CAM calculation and augmentation.</li></ul></li><li><p><strong>Loss Functions and Optimization:</strong></p><ul><li>We employ cross-entropy loss for supervised learning.</li><li>The <code>NCESoftmaxLoss</code> (info-NCE) encourages consistency and distinctiveness in Grad-CAM heatmaps.</li><li>SGD optimizer with momentum and weight decay is used.</li></ul></li><li><p><strong>Training Loop:</strong></p><ul><li>The <code>train</code> function iterates through epochs and mini-batches, performing both supervised and contrastive learning.</li><li>It tracks cross-entropy loss, contrastive loss, and top-1/top-5 accuracies.</li></ul></li><li><p><strong>Grad-CAM Visualization:</strong></p><ul><li>After training, the model is loaded and used to generate Grad-CAM heatmaps for sample images.</li><li>A <code>display_gradcam</code> function visualizes the original images, Grad-CAM masks, and the superimposed results.</li></ul></li><li><p><strong>Model Saving:</strong></p><ul><li>The trained model&rsquo;s state is saved for future use.</li></ul></li></ol><h2 id=resources>Resources
<a class=anchor href=#resources>#</a></h2><ol><li><a href=https://arxiv.org/pdf/2110.00527v2>Original CGC Paper</a></li><li><a href=https://github.com/UCDvision/CGC>Original CGC Implementation</a></li><li><a href=https://arxiv.org/pdf/1512.04150v1>CAM Explanation</a></li><li><a href=https://arxiv.org/pdf/1611.07450>Grad-CAM Explanation</a></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Contrastive%20Grad-CAM%20Consistency.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#consistent-explanations-by-contrastive-learning>Consistent Explanations by Contrastive Learning</a><ul><li><a href=#introduction-unveiling-the-black-box-of-deep-learning>Introduction: Unveiling the Black Box of Deep Learning</a><ul><li><a href=#demystifying-decisions-with-post-hoc-explanations>Demystifying Decisions with Post-hoc Explanations</a></li><li><a href=#the-challenge-of-spatial-transformations>The Challenge of Spatial Transformations</a></li><li><a href=#fine-grained-classification-a-case-for-explainability>Fine-Grained Classification: A Case for Explainability</a></li><li><a href=#suggested-solution-enhancing-grad-cam-for-spatial-consistency>Suggested solution: Enhancing Grad-CAM for Spatial Consistency</a></li><li><a href=#key-ideas-guiding-principles-for-explainability>Key ideas: Guiding Principles for Explainability</a></li><li><a href=#metrics>Metrics</a></li><li><a href=#insights-and-benefits>Insights and Benefits</a></li></ul></li><li><a href=#method>Method</a><ul><li><a href=#background-on-base-explanation-methods>Background on Base Explanation Methods</a></li><li><a href=#contrastive-grad-cam-consistency-loss>Contrastive Grad-CAM Consistency Loss</a></li></ul></li><li><a href=#our-implementation>Our Implementation</a><ul><li><a href=#google-colab-notebookhttpscolabresearchgooglecomdrive1we3vlkexppim5wlimtjh4w_tvi7yikhouspsharing><a href="https://colab.research.google.com/drive/1we3vLKeXpPim5wLiMTjh4w_tVI7YIKHo?usp=sharing">Google Colab Notebook</a></a></li></ul></li><li><a href=#resources>Resources</a></li></ul></li></ul></nav></div></aside></main></body></html>