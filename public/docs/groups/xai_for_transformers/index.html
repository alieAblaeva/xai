<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article &ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&rdquo; by Ameen Ali et.">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/xai_for_transformers/">
  <meta property="og:site_name" content="XAI">
  <meta property="og:title" content="XAI">
  <meta property="og:description" content="XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its’ work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article “XAI for Transformers: Better Explanations through Conservative Propagation” by Ameen Ali et.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Xai for Transformers | XAI</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css" integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c&#43;shO0P7/g/s=" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js" integrity="sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT" crossorigin="anonymous"></script>
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.690e50761b66b2e4cf12ec5df41f0c40cac7f8b58f42e8845ad4158a9deab588.js" integrity="sha256-aQ5QdhtmsuTPEuxd9B8MQMrH&#43;LWPQuiEWtQVip3qtYg=" crossorigin="anonymous"></script>

  <script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk=" crossorigin="anonymous"></script>

  

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/YELLOW_BAR.png" alt="Logo" /><span><b>XAI</b></span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/cam_and_secam/" class="">CAM and SeCAM</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/" class="">Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/" class="">Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/example/" class="">Example</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/ai-playing-geoguessr-explained/" class="">Ai Playing Geo Guessr Explained</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/contrastive-grad-cam-consistency/" class="">Contrastive Grad Cam Consistency</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/dndfs_shap/" class="">Dndfs Shap</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/gradcam/" class="">Grad Cam</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/integrated-gradients/" class="">Integrated Gradients</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/kernel-shap/" class="">Kernel Shap</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/rag/" class="">Rag</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/shap_darya_and_viktoria/" class="">Shap Darya and Viktoria</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/sverl_tac_toe/" class="">Sverl Tac Toe</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/torchprism/" class="">Torch Prism</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/groups/xai_for_transformers/" class="active">Xai for Transformers</a>
  

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Xai for Transformers</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#xai-for-transformers-explanations-through-lrp">XAI for Transformers. Explanations through LRP</a>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#lrp-method">LRP method</a></li>
        <li><a href="#better-lrp-rules-for-transformers">Better LRP Rules for Transformers</a></li>
        <li><a href="#results">Results</a></li>
        <li><a href="#references">References</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="xai-for-transformers-explanations-through-lrp">
  XAI for Transformers. Explanations through LRP
  <a class="anchor" href="#xai-for-transformers-explanations-through-lrp">#</a>
</h1>
<h2 id="introduction">
  Introduction
  <a class="anchor" href="#introduction">#</a>
</h2>
<p>Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article 
  <a href="https://proceedings.mlr.press/v162/ali22a/ali22a.pdf">&ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&rdquo;</a> by Ameen Ali et. al. serves this purpose.</p>
<h2 id="lrp-method">
  LRP method
  <a class="anchor" href="#lrp-method">#</a>
</h2>
<p>Layer-wise Relevance Propagation method here are compared with Gradient×Input method presented in 
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf">earlier article</a> in this field.</p>
<p align="center">
    <img src="/xai_for_transformers/1.png" width='300' alt="LRP outlook"/>
</p>
<h5><font color="DimGray"><center>Img.1. Layer-wise Relevance Propagation principe</center></font></h5>
<p>The relevence in LRP is computing as
$$R(x_i) = \sum_{j} \frac{\delta y_j}{\delta x_i} \frac{x_i}{y_j} R(y_j)$$</p>
<p>But in some layers of transformer formulas look little different. For the attention-head layer and for normalization layers rules are look like</p>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[R(x_i)=\sum_{j}\frac{x_i p_ij}{\sum_{i&#39;} x_{i&#39;} p_{i&#39;j}}R(y_j) \text{ and } R(x_i)=\sum_{j}\frac{x_i (\delta_{ij} - \frac{1}{N})}{\sum_{i&#39;} x_{i&#39;} (\delta_{i&#39;j} - \frac{1}{N})}R(y_j),\]
</span>

<p>where <span>
  \(p_{ij}\)
</span>
 is a gating term value from attention head and for the LayerNorm <span>
  \((\delta_{ij} - \frac{1}{N})\)
</span>
 is the other way of writing the &lsquo;centering matrix&rsquo;, <span>
  \(N\)
</span>
 is the number of tokens in the input sequence.</p>
<h2 id="better-lrp-rules-for-transformers">
  Better LRP Rules for Transformers
  <a class="anchor" href="#better-lrp-rules-for-transformers">#</a>
</h2>
<p>In practice authors observed that these rules do not need to be implemented explicitly. There are trick makes the method straightforward to implement, by adding <code>detach()</code> calls at the appropriate locations in the neural network code and then running standard Gradient×Input.</p>
<p>So improved rules will be
<span>
  \[y_i = \sum_i x_i[p_{ij}].detach()\]
</span>

for every attention head, and
<span>
  \[y_i = \frac{x_i - \mathbb{E}[x]}{\sqrt{\varepsilon &#43; Var[x]}}.detach()\]
</span>

for every LayerNorm, where <span>
  \( \mathbb{E}\)
</span>
 and <span>
  \(Var[x]\)
</span>
 is mean and variance.</p>
<h2 id="results">
  Results
  <a class="anchor" href="#results">#</a>
</h2>
<p>In the article different methods was tested on various datasets, but for now most inetersing is comparisom between old Gradient×Input (GI) method and new LRP methods with modifications in attention head rule (AH), LayerNorm (LN) or both (AH+LN).</p>
<p align="center">
    <img src="/xai_for_transformers/2.png" width='300' alt="GIvsLRP results"/>
</p>
<h5><font color="DimGray"><center>Img.2. AU-MSE (area under the mean squared error)
</center></font></h5>
<p>The LRP with both modifications shows slightly better results in comparison with Gradient×Input method, but may make a huge difference in the future.</p>
<p>The results on SST-2 dataset that contains movie reviews and ratings are shown below. Both transformers was learned to classify review as positive or negative, and LRP shows slightly brighter and more concrete relevance values.</p>
<p align="center">
    <img src="/xai_for_transformers/4.png" alt="SST-2 results"/>
</p>
<h2 id="references">
  References
  <a class="anchor" href="#references">#</a>
</h2>
<p>[1] 
  <a href="https://proceedings.mlr.press/v162/ali22a.html">Ameen Ali et. al. “XAI for Transformers: Better Explanations through Conservative Propagation.” ICML, 2022</a></p>
<p>[2] 
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf">Hila Chefer et. al. “Transformer Interpretability Beyond Attention Visualization.” CVPR, 2021</a></p>
<h2 id="code">
  Code
  <a class="anchor" href="#code">#</a>
</h2>
<p>All code for Transformer you can find in 
  <a href="https://github.com/AmeenAli/XAI_Transformers">https://github.com/AmeenAli/XAI_Transformers</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">




  <div>
    <a class="flex align-center" href="https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/xai_for_transformers.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#xai-for-transformers-explanations-through-lrp">XAI for Transformers. Explanations through LRP</a>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#lrp-method">LRP method</a></li>
        <li><a href="#better-lrp-rules-for-transformers">Better LRP Rules for Transformers</a></li>
        <li><a href="#results">Results</a></li>
        <li><a href="#references">References</a></li>
        <li><a href="#code">Code</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












